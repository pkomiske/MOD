{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import multiprocessing\n",
    "import os\n",
    "import time\n",
    "\n",
    "import energyflow as ef\n",
    "from energyflow.datasets import mod\n",
    "import numpy as np\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cname = 'CMS2011AJets'\n",
    "collection = mod.COLLECTIONS[cname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_file(arg):\n",
    "    filename, (cache_dir, subdir, algorithm) = arg\n",
    "    filepath = ef.utils.data_utils._get_filepath(filename, None, cache_dir, \n",
    "                                                 cache_subdir=subdir, file_hash=None)\n",
    "    return ef.utils.data_utils._hash_file(filepath, algorithm=algorithm)\n",
    "\n",
    "def make_hash_dict(collection, cname, cache_dir='~/.energyflow', compressed=True, algorithm='md5'):\n",
    "    global hashes, name\n",
    "    hashes = {}\n",
    "    for k,v in collection.items():\n",
    "        for dset in v['subdatasets']:\n",
    "            start = time.time()\n",
    "            name, nfiles, record = dset\n",
    "            subdir = os.path.join('datasets', cname, name)\n",
    "            \n",
    "            opts = [cache_dir, subdir, algorithm]\n",
    "            comp_str = '_compressed' if compressed else ''\n",
    "            filenames = ['{}_{}{}.h5'.format(name, i, comp_str) for i in range(nfiles)]\n",
    "            with multiprocessing.Pool() as pool:\n",
    "                for i,h in enumerate(pool.map(hash_file, zip(filenames, nfiles*[opts]))):\n",
    "                    hashes[filenames[i]] = h\n",
    "            \n",
    "            print('Done with {} in {:.3f}s'.format(name, time.time() - start))\n",
    "\n",
    "    return hashes\n",
    "\n",
    "def get_total_weights(collection, cname, cache_dir='~/.energyflow', compressed=True):\n",
    "    weights = {}\n",
    "    for k,v in collection.items():\n",
    "        for dset in v['subdatasets']:\n",
    "            start = time.time()\n",
    "            name, nfiles, record = dset\n",
    "            subdir = os.path.join('datasets', cname, name)\n",
    "            \n",
    "            total_weight = 0.\n",
    "            for i in range(nfiles):\n",
    "                filename = '{}_{}{}.h5'.format(name, i, '_compressed' if compressed else '')\n",
    "                filepath = ef.utils.data_utils._get_filepath(filename, None, cache_dir, \n",
    "                                                         cache_subdir=subdir, file_hash=None)\n",
    "                \n",
    "                dset = mod.MODDataset(filepath, store_pfcs=False, store_gens=False)\n",
    "                total_weight += np.sum(dset.weights)\n",
    "                \n",
    "            weights[name] = total_weight\n",
    "                \n",
    "            print('Done with {} in {:.3f}s'.format(name, time.time() - start))\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashes\n",
      "Done with CMS_Jet300_pT375-infGeV in 0.598s\n",
      "Done with SIM170_Jet300_pT375-infGeV in 0.400s\n",
      "Done with SIM300_Jet300_pT375-infGeV in 0.789s\n",
      "Done with SIM470_Jet300_pT375-infGeV in 1.395s\n",
      "Done with SIM600_Jet300_pT375-infGeV in 1.550s\n",
      "Done with SIM800_Jet300_pT375-infGeV in 1.556s\n",
      "Done with SIM1000_Jet300_pT375-infGeV in 0.965s\n",
      "Done with SIM1400_Jet300_pT375-infGeV in 0.953s\n",
      "Done with SIM1800_Jet300_pT375-infGeV in 0.851s\n",
      "Done with GEN170_pT375-infGeV in 0.353s\n",
      "Done with GEN300_pT375-infGeV in 0.551s\n",
      "Done with GEN470_pT375-infGeV in 1.255s\n",
      "Done with GEN600_pT375-infGeV in 1.364s\n",
      "Done with GEN800_pT375-infGeV in 1.360s\n",
      "Done with GEN1000_pT375-infGeV in 0.856s\n",
      "Done with GEN1400_pT375-infGeV in 0.863s\n",
      "Done with GEN1800_pT375-infGeV in 0.671s\n",
      "\n",
      "Weights\n",
      "Done with CMS_Jet300_pT375-infGeV in 1.800s\n",
      "Done with SIM170_Jet300_pT375-infGeV in 0.021s\n",
      "Done with SIM300_Jet300_pT375-infGeV in 3.760s\n",
      "Done with SIM470_Jet300_pT375-infGeV in 11.672s\n",
      "Done with SIM600_Jet300_pT375-infGeV in 12.623s\n",
      "Done with SIM800_Jet300_pT375-infGeV in 12.744s\n",
      "Done with SIM1000_Jet300_pT375-infGeV in 6.348s\n",
      "Done with SIM1400_Jet300_pT375-infGeV in 6.443s\n",
      "Done with SIM1800_Jet300_pT375-infGeV in 3.183s\n",
      "Done with GEN170_pT375-infGeV in 0.014s\n",
      "Done with GEN300_pT375-infGeV in 2.648s\n",
      "Done with GEN470_pT375-infGeV in 8.064s\n",
      "Done with GEN600_pT375-infGeV in 8.757s\n",
      "Done with GEN800_pT375-infGeV in 8.757s\n",
      "Done with GEN1000_pT375-infGeV in 4.328s\n",
      "Done with GEN1400_pT375-infGeV in 4.397s\n",
      "Done with GEN1800_pT375-infGeV in 2.194s\n"
     ]
    }
   ],
   "source": [
    "print('Hashes')\n",
    "hashes = make_hash_dict(collection, cname)\n",
    "\n",
    "print()\n",
    "print('Weights')\n",
    "weights = get_total_weights(collection, cname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = {'md5_hashes': hashes, 'total_weights': weights}\n",
    "#f = np.load('../../../../../EnergyFlow/energyflow/data/ReweightingFactors.npz')\n",
    "#data.update({k: v.tolist() for k,v in f.items()})\n",
    "#with open('/home/pkomiske/Dropbox/Research/EnergyFlow/energyflow/data/{}.json'.format(cname), 'w') as f:\n",
    "#    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['md5_hashes', 'total_weights', 'kfactor_x', 'kfactor_y', 'npv_hist_ratios', 'residual_factor'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/home/pkomiske/Dropbox/Research/EnergyFlow/energyflow/data/{}.json'.format(cname), 'r') as f:\n",
    "    data = json.load(f)\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
